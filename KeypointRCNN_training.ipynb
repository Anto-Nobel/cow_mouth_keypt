{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"elapsed":3,"status":"error","timestamp":1677947205978,"user":{"displayName":"Anto Nobel","userId":"08673940037299364244"},"user_tz":-330},"id":"UbWr4EhyigsF","outputId":"fa408e94-54a3-418f-aac0-8d8618833ce3"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/mp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is unsupported in this environment.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmountpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: google.colab.drive is unsupported in this environment."]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVkzMKloihH1"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/keypoint_rcnn_training_pytorch')"]},{"cell_type":"markdown","metadata":{"id":"DdtD7MsRieSR"},"source":["# 1. Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANaNh_9oieSS"},"outputs":[],"source":["import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.transforms import functional as F\n","\n","import albumentations as A # Library for augmentations"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"error","timestamp":1677947201882,"user":{"displayName":"Anto Nobel","userId":"08673940037299364244"},"user_tz":-330},"id":"Ym4q-Om7ieST","colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"7b038543-6e60-4991-ce1d-41675db71774"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-985188208ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://github.com/pytorch/vision/tree/main/references/detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transforms'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# https://github.com/pytorch/vision/tree/main/references/detection\n","import transforms, utils, engine, train\n","from utils import collate_fn\n","from engine import train_one_epoch, evaluate"]},{"cell_type":"markdown","metadata":{"id":"UzCOAeQbieST"},"source":["# 2. Augmentations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kYWyGLqieST"},"outputs":[],"source":["def train_transform():\n","    return A.Compose([\n","        A.Sequential([\n","            A.HorizontalFlip(p=0.5),\n","            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n","        ], p=1)\n","    ],\n","    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n","    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n","    )"]},{"cell_type":"markdown","metadata":{"id":"D0fnDy4FieSU"},"source":["# 3. Dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JuWYpTEieSU"},"outputs":[],"source":["class ClassDataset(Dataset):\n","    def __init__(self, root, transform=None, demo=False):                \n","        self.root = root\n","        self.transform = transform\n","        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n","        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n","        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n","    \n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n","        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n","\n","        img_original = cv2.imread(img_path)\n","        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n","        \n","        with open(annotations_path) as f:\n","            data = json.load(f)\n","            bboxes_original = data['bboxes']\n","            keypoints_original = data['keypoints']\n","            \n","            # All objects are glue tubes\n","            bboxes_labels_original = ['Glue tube' for _ in bboxes_original]            \n","\n","        if self.transform:   \n","            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n","            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n","            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n","            # Then we need to convert it to the following list:\n","            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n","            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n","            \n","            # Apply augmentations\n","            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n","            img = transformed['image']\n","            bboxes = transformed['bboxes']\n","            \n","            # Unflattening list transformed['keypoints']\n","            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n","            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n","            # Then we need to convert it to the following list:\n","            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n","            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,2,2)).tolist()\n","\n","            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n","            keypoints = []\n","            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n","                obj_keypoints = []\n","                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n","                    # kp - coordinates of keypoint\n","                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n","                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n","                keypoints.append(obj_keypoints)\n","        \n","        else:\n","            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n","        \n","        # Convert everything into a torch tensor        \n","        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n","        target = {}\n","        target[\"boxes\"] = bboxes\n","        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n","        target[\"image_id\"] = torch.tensor([idx])\n","        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n","        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n","        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)        \n","        img = F.to_tensor(img)\n","        \n","        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n","        target_original = {}\n","        target_original[\"boxes\"] = bboxes_original\n","        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n","        target_original[\"image_id\"] = torch.tensor([idx])\n","        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n","        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n","        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n","        img_original = F.to_tensor(img_original)\n","\n","        if self.demo:\n","            return img, target, img_original, target_original\n","        else:\n","            return img, target\n","    \n","    def __len__(self):\n","        return len(self.imgs_files)"]},{"cell_type":"markdown","metadata":{"id":"TjBGBckBieSW"},"source":["# 4. Visualizing a random item from dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZSDGdJHieSW"},"outputs":[],"source":["KEYPOINTS_FOLDER_TRAIN = '/content/drive/MyDrive/mar4/train'\n","dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n","data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","\n","# iterator = iter(data_loader)\n","# batch = next(iterator)\n","\n","# print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n","# print(\"Transformed targets:\\n\", batch[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yrrWzFcieSX"},"outputs":[],"source":["keypoints_classes_ids2names = {0: 'lower', 1: 'upper'}\n","\n","def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n","    fontsize = 18\n","\n","    for bbox in bboxes:\n","        start_point = (bbox[0], bbox[1])\n","        end_point = (bbox[2], bbox[3])\n","        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n","    \n","    for kps in keypoints:\n","        for idx, kp in enumerate(kps):\n","            image = cv2.circle(image.copy(), tuple(kp), 3, (255,0,0), 10)\n","            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n","\n","    if image_original is None and keypoints_original is None:\n","        plt.figure(figsize=(40,40))\n","        plt.imshow(image)\n","\n","    else:\n","        for bbox in bboxes_original:\n","            start_point = (bbox[0], bbox[1])\n","            end_point = (bbox[2], bbox[3])\n","            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n","        \n","        for kps in keypoints_original:\n","            for idx, kp in enumerate(kps):\n","                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 10)\n","                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n","\n","        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n","\n","        ax[0].imshow(image_original)\n","        ax[0].set_title('Original image', fontsize=fontsize)\n","\n","        ax[1].imshow(image)\n","        ax[1].set_title('Transformed image', fontsize=fontsize)\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFOr7NESFSJ_"},"outputs":[],"source":["iterator = iter(data_loader)\n","batch = next(iterator) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgJMosuqFXp3"},"outputs":[],"source":["#while batch!=null: \n","for i in range(10):\n","  image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n","  bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n","\n","  keypoints = []\n","  for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n","      keypoints.append([kp[:2] for kp in kps])\n","\n","  image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n","  bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n","\n","  keypoints_original = []\n","  for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n","      keypoints_original.append([kp[:2] for kp in kps])\n","\n","  visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original) \n","  batch = next(iterator)"]},{"cell_type":"markdown","metadata":{"id":"2Wzh2VJ1ieSX"},"source":["# 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3Md533xieSX"},"outputs":[],"source":["def get_model(num_keypoints, weights_path=None):\n","    \n","    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n","    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n","                                                                   pretrained_backbone=True,\n","                                                                   num_keypoints=num_keypoints,\n","                                                                   num_classes = 2, # Background is the first class, object is the second class\n","                                                                   rpn_anchor_generator=anchor_generator)\n","\n","    if weights_path:\n","        state_dict = torch.load(weights_path)\n","        model.load_state_dict(state_dict)        \n","        \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9MGWdHezP5e"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbAOLKy4zW2m"},"outputs":[],"source":["KEYPOINTS_FOLDER_TRAIN = '/content/drive/MyDrive/mar4/train'\n","KEYPOINTS_FOLDER_TEST = '/content/drive/MyDrive/mar4/test'\n","\n","dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n","dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n","\n","data_loader_train = DataLoader(dataset_train, batch_size=3, shuffle=True, collate_fn=collate_fn)\n","data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","# model = get_model(num_keypoints = 2,weights_path='/content/drive/MyDrive/keypoint_rcnn_training_pytorch/mar/keypointsrcnn_weights_mar4_50.pth')\n","model = get_model(num_keypoints = 2)\n","model.to(device)"]},{"cell_type":"code","source":["def save_checkpoint(state, epoch):\n","    torch.save(state,'/content/drive/MyDrive/keypoint_rcnn_training_pytorch/keypointsrcnn_weights/mar4/epoch_'+str(epoch)+'.pth.tar)"],"metadata":{"id":"Sw_GsezanAjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qtLFzs3ieSY"},"outputs":[],"source":["params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n","num_epochs = 50\n","\n","for epoch in range(num_epochs):\n","    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n","    lr_scheduler.step()\n","    evaluate(model, data_loader_test, device) \n","    save_checkpoint({\n","            'epoch': epoch + 1,\n","            'arch': args.arch,\n","            'state_dict': model.state_dict(),\n","            'best_prec1': best_prec1,\n","            'optimizer' : optimizer.state_dict(),\n","        },epoch)\n","# Save model weights after training\n","# torch.save(model.state_dict(), 'keypointsrcnn_weights.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFp8qRIcQ0I6"},"outputs":[],"source":["# torch.save(model.state_dict(), '/content/drive/MyDrive/keypoint_rcnn_training_pytorch/keypointsrcnn_weights_feb28_50.pth')"]},{"cell_type":"markdown","metadata":{"id":"DwtJAjvcieSY"},"source":["# 6. Visualizing model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqDU4w2CieSY"},"outputs":[],"source":["iterator = iter(data_loader_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcxB6h5DNVVU"},"outputs":[],"source":["images, targets = next(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3OwrOodieSY"},"outputs":[],"source":["\n","images = list(image.to(device) for image in images)\n","\n","with torch.no_grad():\n","    model.to(device)\n","    model.eval()\n","    output = model(images)\n","\n","# print(\"Predictions: \\n\", output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wjMM1INieSZ"},"outputs":[],"source":["image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n","scores = output[0]['scores'].detach().cpu().numpy()\n","\n","high_scores_idxs = np.where(scores > 0.5)[0].tolist() \n","post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n","\n","\n","keypoints = []\n","for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n","    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n","\n","bboxes = []\n","for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n","    bboxes.append(list(map(int, bbox.tolist())))\n","    \n","visualize(image, bboxes, keypoints)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}